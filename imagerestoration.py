# -*- coding: utf-8 -*-
"""imagerestoration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GfRcrTV0NGHQHCyzKaKWAkmao1gEM0fh
"""

!pip install tensorflow==2.4

#import tensorflow and other libraries

import tensorflow as tf
import os
import time
from matplotlib import pyplot as plt
from IPython import display

!pip install -U tensorboard

from google.colab import drive
drive.mount('/content/drive')

#load dataset

!git clone https://github.com/PieterBijl/Group28.git

PATH = '/content/Group28/datasets/variational_data/'
PATH

BUFFER_SIZE = 400
BATCH_SIZE = 1
IMG_WIDTH = 256
IMG_HEIGHT = 256

def load(image_file):
  image = tf.io.read_file(image_file)
  image = tf.image.decode_png(image)

  w=tf.shape(image)[1]

  w=w//2

  input_image=image[:,:w,:]
  real_image=image[:,w:,:]

  input_image=tf.cast(input_image,tf.float32)
  real_image=tf.cast(real_image,tf.float32)

  return input_image, real_image

inp, re=load(PATH+'train/002.png')
#casting to int for matplotlib to show the image
plt.figure()
plt.imshow(inp/255.0)
plt.figure()
plt.imshow(re/255.0)

def resize(input_image,real_image,height,width):
  input_image=tf.image.resize(input_image,[height,width],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
  real_image=tf.image.resize(real_image,[height,width],method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)

  return input_image,real_image

def random_crop(input_image,real_image):
  stacked_image=tf.stack([input_image,real_image],axis=0)
  cropped_image=tf.image.random_crop(stacked_image,size=[2,IMG_HEIGHT,IMG_WIDTH,3])

  return cropped_image[0],cropped_image[1]

#normalizing the images to [-1,1]

def normalize(input_image,real_image):
  input_image=(input_image/127.5) - 1
  real_image=(real_image/127.5) - 1

  return input_image,real_image

@tf.function()
def random_jitter(input_image,real_image):
  #resizing to 286x286x3
  input_image,real_image=resize(input_image,real_image,286,286)

  #randomly cropping to 256x256x3
  input_image,real_image=random_crop(input_image,real_image)

  if tf.random.uniform(())>0.5:
    #random mirroring
    input_image=tf.image.flip_left_right(input_image)
    real_image=tf.image.flip_left_right(real_image)

  return input_image, real_image

plt.figure(figsize=(6,6))
for i in range(4):
  rj_inp,rj_re=random_jitter(inp,re)
  plt.subplot(2,2,i+1)
  plt.imshow(rj_inp/255.0)
  plt.axis('off')
  plt.show

def load_image_train(image_file):
  input_image,real_image=load(image_file)
  input_image,real_image=random_jitter(input_image,real_image)
  input_image,real_image=normalize(input_image,real_image)

  return input_image,real_image

def load_image_test(image_file):
  input_image,real_image=load(image_file)
  input_image,real_image=resize(input_image,real_image,IMG_HEIGHT,IMG_WIDTH)
  input_image,real_image=normalize(input_image,real_image)

  return input_image,real_image

#INPUT PIPELINE

train_dataset=tf.data.Dataset.list_files(PATH+'train/*.png')
train_dataset=train_dataset.map(load_image_train,num_parallel_calls=tf.data.AUTOTUNE)

train_dataset=train_dataset.shuffle(BUFFER_SIZE)
train_dataset=train_dataset.batch(BATCH_SIZE)

train_dataset

test_dataset=tf.data.Dataset.list_files(PATH+'test/*.png')
test_dataset=test_dataset.map(load_image_test)

test_dataset=train_dataset.batch(BATCH_SIZE)

test_dataset

#BUILD THE GENERATOR

OUTPUT_CHANNELS=3

def downsample(filters,size,apply_batchnorm=True):
  initializer=tf.random_normal_initializer(0.,0.02)

  result=tf.keras.Sequential()
  result.add(tf.keras.layers.Conv2D(filters,size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False))

  if apply_batchnorm:
    result.add(tf.keras.layers.BatchNormalization())

    result.add(tf.keras.layers.LeakyReLU())

    return result

down_model=downsample(3,4)
down_result=down_model(tf.expand_dims(inp,0))
print(down_result.shape)

def upsample(filters,size,apply_dropout=False):
  initializer=tf.random_normal_initializer(0.,0.02)

  result=tf.keras.Sequential()
  result.add(tf.keras.layers.Conv2DTranspose(filters,size,strides=2,padding='same',kernel_initializer=initializer,use_bias=False))

  result.add(tf.keras.layers.BatchNormalization())

  if apply_dropout:
    result.add(tf.keras.layers.Dropout(0.5))

  result.add(tf.keras.layers.ReLU())

  return result

up_model=upsample(3,4)
up_result=up_model(down_result)
print(up_result.shape)

def Generator():
  inputs=tf.keras.layers.Input(shape=[256,256,3])

  down_stack=[
        downsample(64,4),
        downsample(128,4),
        downsample(256,4),
        downsample(512,4),
        downsample(512,4),
        downsample(512,4),
        downsample(512,4),
        downsample(512,4)
  ]

  up_stack = [
              upsample(512,4,apply_dropout=True),
              upsample(512,4,apply_dropout=True),
              upsample(512,4,apply_dropout=True),
              upsample(512,4),
              upsample(256,4),
              upsample(128,4),
              upsample(64,4)
  ]

  initializer=tf.random_normal_initializer(0.,0.02)
  last=tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS,4,strides=2,padding='same',kernel_initializer=initializer,activation='tanh')

  x=inputs
  print(x)
  print(down_stack)

  #downsampling through the model
  skips=[]
  for down in down_stack:
    print(down)
    x=down(x)
    skips.append(x)

  skips=reversed(skips[:-1])

  #upsampling and establishing the skip connections

  for up,skip in zip(up_stack,skips):
    x=up(x)
    x=tf.keras.layers.Concatenate()([x,skip])

  x=last(x)

  return tf.keras.Model(inputs=inputs,outputs=x)

generator = Generator()
tf.keras.utils.plot_model(generator,show_shapes=True,dpi=64)

gen_output=generator(inp[tf.newaxis,...],training=False)
plt.imshow(gen_output[0,...])

LAMBA=100

def generator_loss(disc_generated_output,gen_output,target):
  gan_loss=loss_object(tf.ones_like(disc_generated_output),disc_generated_output)

  #mean absolute error
  l1_loss=tf.reduce_mean(tf.abs (targer - gen_output))

  total_gen_loss=gan_loss+(LAMBA*l1_loss)

  return total_gen_loss,gan_loss,l1_loss

#discriminator

def Discriminator():
  initializer=tf.random_normal_initializer(0.,0.02)

  inp=tf.keras.layers.Input(shape=[256,256,3],name='input_image')
  tar=tf.keras.layers.Input(shape=[256,256,3],name='target_image')

  x=tf.keras.layers.concatenate([inp,tar])

  down1=downsample(64,4)(x)
  down2=downsample(128,4)(down1)
  down3=downsample(256,4)(down2)

  zero_pad1=tf.keras.layers.ZeroPadding2D()(down3)
  conv=tf.keras.layers.Conv2D(512,4,strides=1,kernel_initializer=initializer,use_bias=False)(zero_pad1)

  batchnorm1=tf.keras.layers.BatchNormalization()(conv)
  leaky_relu=tf.keras.layers.LeakyReLU()(batchnorm1)
  zero_pad2=tf.keras.layers.ZeroPadding2D()(leaky_relu)
  last=tf.keras.layers.Conv2D(1,4,strides=1,kernel_initializer=initializer)(zero_pad2)

  return tf.keras.Model(inputs=[inp,tar],outputs=last)

discriminator=Discriminator()
tf.keras.utils.plot_model(discriminator,show_shapes=True,dpi=64)